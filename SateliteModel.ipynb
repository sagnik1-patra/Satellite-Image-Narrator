{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656b15e8-6661-4e27-b110-bf7c6c1ed4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ce69aa7d394d39a8f5fcf7d4431e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sagni\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d669f3691a84dcda0d731487f84d8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fe51987101410fb7b62dc7dc16ec7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4b9254249342d08c957d34c950bb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa970bbfc734f0bb1ffd5986f5548a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac4eff991e04402a978f94edf448f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133b0d8ee4f54610a1c4a4da6984762d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ›°ï¸ NDVI Narration:\n",
      "--------------------------------------------------\n",
      "\n",
      "The satellite NDVI change analysis over a region of the Amazon shows:\n",
      "- Average NDVI change: 0.394\n",
      "- Maximum NDVI gain: 1.000\n",
      "- Maximum NDVI loss: 0.000\n",
      "- Percentage of area with significant vegetation loss (NDVI < -0.2): 0.00%\n",
      "\n",
      "Generate a short, human-readable environmental report summarizing this in plain English.\n",
      "\n",
      "Export citation and information about the publisher\n",
      "\n",
      "This article is available as a free compendium in the print edition of the EO.\n",
      "\n",
      "Econometrica\n",
      "\n",
      "This article is available as a free citation and access for the EO.\n",
      "\n",
      "Introduction\n",
      "\n",
      "Econometrica, a comprehensive, open source, open source, and open source numerical approach, is a computational tool used in numerical analysis for the detection of natural and anthropogenic climate change. It is a scientific framework that can be used to obtain realistic measurements of climate change. It is available for use in both computer science and engineering.\n",
      "\n",
      "The EO has been developed for the purpose of estimating the global mean surface temperature (MWT) using climate models. It is the most widely used climate modelling tool in the world. The EO has been used to create many new climate models which have been developed for the purpose of determining the most suitable for climate change. With the EO, researchers could apply it to many different climate models and then compare them against each other.\n",
      "\n",
      "The EO is based on the assumption that the global mean surface temperature (MWT) is constant over a range of time (e.g., 1.5â€“4.5Â°C). The\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import joblib\n",
    "import yaml\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "# === Step 1: Load Data ===\n",
    "base_path = r\"C:\\Users\\sagni\\Downloads\\Satellite Image Narrator\"\n",
    "h5_path = f\"{base_path}\\\\ndvi_change_amazon.h5\"\n",
    "yaml_path = f\"{base_path}\\\\ndvi_change_amazon.yaml\"\n",
    "pkl_path = f\"{base_path}\\\\ndvi_change_amazon.pkl\"\n",
    "\n",
    "# Load NDVI data from h5 file\n",
    "with h5py.File(h5_path, 'r') as f:\n",
    "    # Try common keys\n",
    "    dataset_name = list(f.keys())[0]\n",
    "    ndvi_data = f[dataset_name][:]\n",
    "    ndvi_data = np.nan_to_num(ndvi_data, nan=0.0)\n",
    "\n",
    "# Load metadata (use FullLoader for tuples etc.)\n",
    "with open(yaml_path, 'r') as f:\n",
    "    metadata = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Load scaler (if needed later)\n",
    "scaler = joblib.load(pkl_path)\n",
    "\n",
    "# === Step 2: Compute NDVI Stats ===\n",
    "mean_ndvi = np.mean(ndvi_data)\n",
    "min_ndvi = np.min(ndvi_data)\n",
    "max_ndvi = np.max(ndvi_data)\n",
    "neg_pixels = np.sum(ndvi_data < -0.2)\n",
    "total_pixels = ndvi_data.size\n",
    "loss_percent = (neg_pixels / total_pixels) * 100\n",
    "\n",
    "# === Step 3: Generate NLP Narration ===\n",
    "prompt = f\"\"\"\n",
    "The satellite NDVI change analysis over a region of the Amazon shows:\n",
    "- Average NDVI change: {mean_ndvi:.3f}\n",
    "- Maximum NDVI gain: {max_ndvi:.3f}\n",
    "- Maximum NDVI loss: {min_ndvi:.3f}\n",
    "- Percentage of area with significant vegetation loss (NDVI < -0.2): {loss_percent:.2f}%\n",
    "\n",
    "Generate a short, human-readable environmental report summarizing this in plain English.\n",
    "\"\"\"\n",
    "\n",
    "# Use transformers to generate explanation\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "result = generator(prompt, max_length=120, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "# === Step 4: Output ===\n",
    "print(\"ðŸ›°ï¸ NDVI Narration:\")\n",
    "print(\"-\" * 50)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afd1d9-67c2-4873-937a-cdf29237299d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
